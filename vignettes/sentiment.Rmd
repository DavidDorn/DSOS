---
title: "Sentimentanalyse"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{sentimentanalyse}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Sentiment Analyse

Häufig ist nicht nur das Volumen von Tweets von Interesse sondern auch was diese Tweets zum Ausdruck bringen. Ein wichtiger Teil ist as sogenannte Sentiment, häufig auch als Valenz oder Polarität bezeichnet. Alle drei Begriffe bezeichnen in diesem Fall die Einstellung des Verfassers über das Objekt, auf das sich die Aussage bezieht. Die quantitative Auswertung der Polarität wird oft als Sentiment Analyse bezeichnet. Es gibt verschiedenste Ansätze, welche sich grob in zwei Überkategorien einteilen lassen: Lexikon-basierte und machine learning basierte. Im ersten Fall werden Lexika verwendet, welche von Menschen zusammengestellt wurden und Wörter oder Wortgebilde beinhalten, denen jeweils eine Polarität zugewiesen wurde. Wenn man dann einen gegebenen Text untersuchen möchte, wird einfach jedem Wort der zugehörige Polaritätswert zugewiesen zu einem Gesamtsentiment des betrachteten Textes zusammengefasst (in der Realität ist es natürlich komplexer, es wird gewichtet und komplexere Zusammenhänge wie bspw. Negatoren ("nicht schlecht" etc.) werden berücksichtigt usw.). Der machine learning Ansatz betrachtet nicht die einzelnen Wörter sondern den Text insgesamt. Den einzelnen Texten werden dann von menschlichen "Kodierern" Polaritätswerte zugewiesen (kategorial oder per metrischer Scala) und es wird ein Modell darauf trainiert, diese Werte vorherzusagen. Der Nachteil ist, dass hierfür riesige Mengen (Minimum in die 10.000er, je mehr desto besser) so "gelabelter" Beobachtungen von Nöten sind um eine einigermaßen gute Vorhersagegüte zu bekommen und Menschen gar nicht mal so gut sind, die Polarität von Texten einigermaßen zuverlässig und konsistent zu identifizieren und zusammenzufassen. Die besten Ansätze sind in der Regel Kombinationen aus beidem und regelbasierten Ansätzen (sprich hart-kodierte Zusammenhänge), insgesamt kann man aber sagen, dass wirklich akkurates Sentiment Scoring alles andere als simpel ist (insbesondere bei Tweets).

Wir beschränken uns hier zunächst auf Lexikon-basierte Ansätze wie sie bspw. im sentimentr Paket implementiert sind. 

## Das sentimentr Paket

Das Paket `sentimentr` enthält Funktionen die uns dabei helfen, Tweets einen Sentiment Score zuzuweisen. 

```{r}
library(rtweet)
library(sentimentr)

tweets <- search_tweets("iphone", 1000, lang = "en")

tweet_sentiment <- get_sentences(tweets$text) %>% sentiment_by()

head(tweet_sentiment)

```


Ich kann mir die Tweets mit markiertem Sentiment anzeigen lassen, so kann ich sehen was mir das Programm wie gekeinnzeichnet hat und die Güte einschätzen:

```{r, eval = FALSE}
sentimentr::highlight(tweet_sentiment)

```

Wie ihr seht ist das Ganze nicht wirklich akkurat. Das ist an sich nicht unbedingt schlimm, wenn es uns bspw. nur darum geht, Unterschiede in **aggregiertem** Sentiment zu betrachten, solange wir zumindest davon ausgehen können, dass alle Werte mit gleicher Wahrscheinlichkeit in die selbe Richtung verzerrt sind. Letzteres ist insbesondere dann nicht der Fall, wenn unser Untersuchungsgegenstand Wörter beinhaltet die selber ein Polaritätsscoring besitzen (bei Filmen etc. oft der Fall). Diese müsste man dann vor dem Scoring entfernen. Um wirklich aussagekräftigere Ergebnisse zu bekommen müsste man die Tweets natürlich noch erheblicher bereinigen. Wie in allen Belangen des Lebens gilt auch hier "garbage in, garbage out".

### Andere Funktionen

Das Sentimentr Paket beinhaltet noch eine Reihe weiterer Funktionen die mir im Umgang mit Sentiment-gescoreten Texten erleichtern können. Bspw. kann ich mir das Sentimen über die Länge des Textes anzeigen lassen (was bei Tweets nicht wirklich viel Sinn ergibt):

```{r}
plot(tweet_sentiment)
```


## Allgemein mit Text arbeiten

Wenn wir allgemein mit Text-Daten arbeiten, bspw. um tweets zu bereinigen, sind einige Pakete sehr hilfreich. Eines davon ist das `tidytext` Paket ( [hier](https://www.tidytextmining.com/index.html) findet ihr ein, wie ich finde, sehr gutes Buch dazu). In diesem sind beispielsweise sogenannte _stop words_ für die englische Sprache enthalten. Das sind Wörter die bspw. besonders häufig vorkommen und deshalb keine Aussagekraft beinhalten und für Analysen irrelevant sind.

```{r}
library(tidytext)

data("stop_words")
stop_words

```

Wir können uns den Unterschied veranschaulichen. Ohne Stopwords sehen wir ganz oben vor allem gängige Wörter der englischen Sprache wie "the", "to" usw.:

```{r}
tokenized_tweets <- tweets %>% select(status_id, text) %>% unnest_tokens(word, text)

tokenized_tweets %>% count(word, sort = T)

```

Diese können wir entfernen, indem wir gegen eine Stop-Word Liste "gegenchecken", dies geht ganz gut mit dem anti-join() Befehl:

```{r}
tokenized_tweets %>% anti_join(stop_words) %>% count(word, sort = T)
```


Wie ihr seht, haben wir immer noch "Wörter" drin die wie "Beifang" aussehen: "https", "t.co", usw. Dies sind Twitter-spezifische "Wörter", die wir auch noch herausfiltern können indem wir bspw. eine maßgeschneiderte Liste mit stop words pflegen. 

```{r}
custom_stopwords <- tibble(lexicon = "custom", word = c("t.co", "https"))

```

Wenn wir die Tweets einigermaßen bereinigt haben können wir uns schonmal eine schöne Wordcloud basteln:


```{r}
library(wordcloud)

tokenized_tweets %>%
  anti_join(stop_words) %>%
  anti_join(custom_stopwords) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 50))

```








